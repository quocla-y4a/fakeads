{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84beefb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy autogluon geopy underthesea requests beautifulsoup4 regex tqdm sentencepiece transformers torch\n",
    "# Nếu chạy trên colab / local, thêm: pip install sentencepiece transformers torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df7ada8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fadaml_jobs.py\n",
    "import re, os, json, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# NLP / NER helpers\n",
    "try:\n",
    "    from underthesea import ner, word_tokenize\n",
    "except Exception:\n",
    "    ner = None\n",
    "    def word_tokenize(s): return s.split()\n",
    "\n",
    "# Geo helper (optional)\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "# AutoGluon\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "\n",
    "# ---------- 1) Crawl or load data ----------\n",
    "# Option A: load CSV you already have\n",
    "DATA_PATH = \"job_posts.csv\"  # <-- đổi thành file job của bạn\n",
    "df = pd.read_csv(DATA_PATH)   # expecting columns: 'raw_text' or 'description', maybe 'label' if you have\n",
    "\n",
    "# Option B: (template) simple scraper for a job-listing page (example)\n",
    "# (uncomment and adapt if you need to crawl)\n",
    "\"\"\"\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "def crawl_job_page(url):\n",
    "    r = requests.get(url, timeout=10)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    # Example selectors: adapt per site\n",
    "    title = soup.select_one('h1') and soup.select_one('h1').get_text(strip=True)\n",
    "    desc = soup.select_one('.job-description') and soup.select_one('.job-description').get_text(' ', strip=True)\n",
    "    company = soup.select_one('.company') and soup.select_one('.company').get_text(strip=True)\n",
    "    location = soup.select_one('.location') and soup.select_one('.location').get_text(strip=True)\n",
    "    return {'title': title, 'description': desc, 'company': company, 'location': location}\n",
    "\"\"\"\n",
    "\n",
    "# ---------- 2) Preprocessing ----------\n",
    "def clean_text(s):\n",
    "    if pd.isna(s): return \"\"\n",
    "    s = str(s)\n",
    "    s = re.sub(r'<.*?>', ' ', s)                # remove html tags\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    s = s.lower()\n",
    "    return s\n",
    "\n",
    "text_col = None\n",
    "for candidate in ['description','raw_text','text','job_description']:\n",
    "    if candidate in df.columns:\n",
    "        text_col = candidate\n",
    "        break\n",
    "if text_col is None:\n",
    "    raise RuntimeError(\"Không tìm cột text — đổi tên cột chứa job description thành 'description' or 'raw_text'\")\n",
    "\n",
    "df['description_clean'] = df[text_col].apply(clean_text)\n",
    "\n",
    "# ---------- 3) NER + Tabular feature extraction ----------\n",
    "# We'll extract: salary, location, company, job_title, employment_type, experience, remote_flag, skills_count\n",
    "salary_regex = re.compile(r'(\\d+(?:[.,]\\d+)?\\s*(?:tỷ|triệu|vnđ|vnd|k|m|usd|\\$))', flags=re.I)\n",
    "salary_num_regex = re.compile(r'(\\d[\\d\\.,]*)')\n",
    "\n",
    "def extract_salary(s):\n",
    "    m = salary_regex.search(s)\n",
    "    if not m: return np.nan\n",
    "    num = salary_num_regex.search(m.group(1))\n",
    "    if not num: return np.nan\n",
    "    raw = num.group(1).replace(',','').replace('.','')\n",
    "    try:\n",
    "        v = float(raw)\n",
    "    except:\n",
    "        return np.nan\n",
    "    # heuristics: if contains 'tỷ' -> *1e3 (store in million VND)\n",
    "    if 'tỷ' in m.group(1):\n",
    "        return v * 1000.0\n",
    "    if 'triệu' in m.group(1) or 'm' in m.group(1):\n",
    "        return v\n",
    "    if 'k' in m.group(1):\n",
    "        return v * 0.001\n",
    "    # USD -> you may convert externally\n",
    "    return v\n",
    "\n",
    "def extract_remote(s):\n",
    "    return int(bool(re.search(r'remote|làm việc từ xa|work from home|wfh', s, flags=re.I)))\n",
    "\n",
    "def extract_job_title(s, fallback=None):\n",
    "    # Try simple heuristics: line start; or field 'title' if exists in df\n",
    "    return fallback if fallback else ''\n",
    "\n",
    "# use underthesea NER if available (Vietnamese)\n",
    "def extract_company_location_experience(s):\n",
    "    ent = {}\n",
    "    if ner:\n",
    "        try:\n",
    "            res = ner(s)  # underthesea ner returns list of (token, label)\n",
    "            # naive: join tokens labeled ORG -> company, LOC -> location\n",
    "            orgs = []\n",
    "            locs = []\n",
    "            for token, label in res:\n",
    "                if label == 'ORG': orgs.append(token)\n",
    "                if label == 'LOC': locs.append(token)\n",
    "            ent['company'] = ' '.join(orgs) if orgs else None\n",
    "            ent['location'] = ' '.join(locs) if locs else None\n",
    "        except Exception:\n",
    "            ent['company'] = None\n",
    "            ent['location'] = None\n",
    "    else:\n",
    "        ent['company'] = None\n",
    "        ent['location'] = None\n",
    "    # experience level heuristic\n",
    "    if re.search(r'senior|sr|trên 5 năm|5\\+ năm|thâm niên', s, flags=re.I):\n",
    "        ent['experience_level'] = 'senior'\n",
    "    elif re.search(r'junior|fresh|mới tốt nghiệp|0-1 năm', s, flags=re.I):\n",
    "        ent['experience_level'] = 'junior'\n",
    "    elif re.search(r'mid|2-4 năm|trên 2 năm|3 năm', s, flags=re.I):\n",
    "        ent['experience_level'] = 'mid'\n",
    "    else:\n",
    "        ent['experience_level'] = None\n",
    "    return ent\n",
    "\n",
    "tq = tqdm(df['description_clean'].fillna('').tolist(), desc='extract features', total=len(df))\n",
    "companies, locations, experiences, salaries = [], [], [], []\n",
    "remote_flags, skills_counts = [], []\n",
    "for s in tq:\n",
    "    salaries.append(extract_salary(s))\n",
    "    remote_flags.append(extract_remote(s))\n",
    "\n",
    "    ent = extract_company_location_experience(s)\n",
    "    companies.append(ent.get('company'))\n",
    "    locations.append(ent.get('location'))\n",
    "    experiences.append(ent.get('experience_level'))\n",
    "\n",
    "    # skills_count: count occurrences of 'skill-like' words (simple)\n",
    "    skills = re.findall(r'\\b(python|java|sql|excel|aws|docker|react|node|ml|ai|tensorflow|pytorch)\\b', s, flags=re.I)\n",
    "    skills_counts.append(len(set(skills)))\n",
    "\n",
    "df['salary_million'] = salaries\n",
    "df['is_remote'] = remote_flags\n",
    "df['company_extracted'] = companies\n",
    "df['location_extracted'] = locations\n",
    "df['experience_level'] = experiences\n",
    "df['skills_count'] = skills_counts\n",
    "\n",
    "# ---------- 4) Feature enrichment (optional geospatial) ----------\n",
    "# If you have location strings, use GeoPy to get lat/lon, then nearest big-city features\n",
    "geolocator = Nominatim(user_agent=\"fadaml_jobs_geocoder\")\n",
    "def geocode_safe(loc):\n",
    "    try:\n",
    "        if pd.isna(loc): return (np.nan,np.nan)\n",
    "        res = geolocator.geocode(loc + \", Vietnam\", timeout=10)\n",
    "        if not res: return (np.nan,np.nan)\n",
    "        return (res.latitude, res.longitude)\n",
    "    except Exception:\n",
    "        return (np.nan,np.nan)\n",
    "\n",
    "# Be careful: geocoding many rows can be slow + rate-limited. Do only if needed and cache results.\n",
    "if 'location_extracted' in df.columns:\n",
    "    geocache = {}\n",
    "    latitudes, longitudes = [], []\n",
    "    for loc in tqdm(df['location_extracted'].fillna('').unique(), desc='geocoding unique locs'):\n",
    "        if loc=='':\n",
    "            geocache[loc] = (np.nan,np.nan)\n",
    "        else:\n",
    "            geocache[loc] = geocode_safe(loc)\n",
    "    for loc in df['location_extracted'].fillna(''):\n",
    "        lat,lon = geocache.get(loc,(np.nan,np.nan))\n",
    "        latitudes.append(lat); longitudes.append(lon)\n",
    "    df['lat'] = latitudes; df['lon'] = longitudes\n",
    "\n",
    "# ---------- 5) Data cleaning, outlier removal, dedup ----------\n",
    "# Dedup by description text\n",
    "df = df.drop_duplicates(subset=['description_clean'])\n",
    "# Remove entries lacking core info (optionally)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# ---------- 6) Prepare dataset for AutoGluon ----------\n",
    "# Map to features list (similar role as Table 4 in paper)\n",
    "features = [\n",
    "    'description_clean',  # text\n",
    "    'salary_million',      # numeric\n",
    "    'company_extracted',   # categorical\n",
    "    'location_extracted',\n",
    "    'experience_level',\n",
    "    'is_remote',\n",
    "    'skills_count',\n",
    "    'lat','lon'            # optional geospatial\n",
    "]\n",
    "# If you have label column, keep it. Otherwise assume you will create label by heuristic.\n",
    "label_col = 'label'  # expected 0/1 where 1 = fake (adapt)\n",
    "if label_col not in df.columns:\n",
    "    # Heuristic labelling example: salary extremely low/high vs median for location/job -> mark suspect\n",
    "    df['salary_million_filled'] = df['salary_million'].fillna(df['salary_million'].median())\n",
    "    median_by_loc = df.groupby('location_extracted')['salary_million_filled'].median().replace({np.nan:df['salary_million_filled'].median()})\n",
    "    df['median_loc'] = df['location_extracted'].map(median_by_loc)\n",
    "    # label = 1 if salary deviates > 4x or missing critical info (no company + no location)\n",
    "    df['label'] = ((df['salary_million_filled'] > 4 * df['median_loc']) | (df['salary_million_filled'] < 0.25 * df['median_loc'])) | (df['company_extracted'].isna() & df['location_extracted'].isna())\n",
    "    df['label'] = df['label'].astype(int)\n",
    "\n",
    "train_df = df[features + ['label']].copy()\n",
    "train_df = train_df.sample(frac=1.0, random_state=42).reset_index(drop=True)  # shuffle\n",
    "\n",
    "# convert to TabularDataset\n",
    "train_tab = TabularDataset(train_df)\n",
    "\n",
    "# ---------- 7) Train AutoGluon (mimic paper: ngram (1,3), max_features 10000, memory ratio 0.15) ----------\n",
    "save_path = \"Autogluon_FADAML_jobs\"  # where models stored\n",
    "predictor = TabularPredictor(label='label', path=save_path, eval_metric='f1').fit(\n",
    "    train_data=train_tab,\n",
    "    presets=['best_quality'],  # mimics strong config; can change to 'medium_quality_faster_train' for speed\n",
    "    # you can pass hyperparameters to force certain models or control n-gram; using defaults often ok\n",
    "    time_limit=3600  # optional: allow up to 1 hour; change as you need\n",
    ")\n",
    "\n",
    "# ---------- 8) Evaluate ----------\n",
    "# If you have holdout split\n",
    "from sklearn.metrics import classification_report\n",
    "train_eval = predictor.evaluate(train_tab)\n",
    "y_pred = predictor.predict(train_tab)\n",
    "print(classification_report(train_tab['label'], y_pred))\n",
    "\n",
    "# ---------- 9) Feature importance (permutation) and ablation ----------\n",
    "fi = predictor.feature_importance(train_tab)\n",
    "print(\"Feature importance (top 20):\")\n",
    "print(fi.head(20))\n",
    "\n",
    "# Ablation study example: remove geospatial features, retrain quickly (short time_limit)\n",
    "ablation_train = train_tab.drop(columns=['lat','lon'])\n",
    "predictor_ab = TabularPredictor(label='label', path=save_path + \"_no_geo\", eval_metric='f1').fit(\n",
    "    train_data=ablation_train,\n",
    "    presets=['medium_quality'],\n",
    "    time_limit=600\n",
    ")\n",
    "print(\"Ablation (no geo) eval:\")\n",
    "print(predictor_ab.evaluate(ablation_train))\n",
    "\n",
    "# Save processed dataset for inspection\n",
    "train_df.to_csv(\"processed_jobs_multimodal.csv\", index=False)\n",
    "print(\"Saved processed dataset -> processed_jobs_multimodal.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
