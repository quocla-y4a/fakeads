{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Topic"
      ],
      "metadata": {
        "id": "6WPadQIsAS9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install youtube_transcript_api"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8DUvQXzAeAN",
        "outputId": "1681b032-1703-4779-a5fe-2db061f129a4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting youtube_transcript_api\n",
            "  Downloading youtube_transcript_api-1.2.2-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from youtube_transcript_api) (0.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from youtube_transcript_api) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->youtube_transcript_api) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->youtube_transcript_api) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->youtube_transcript_api) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->youtube_transcript_api) (2025.8.3)\n",
            "Downloading youtube_transcript_api-1.2.2-py3-none-any.whl (485 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m485.0/485.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: youtube_transcript_api\n",
            "Successfully installed youtube_transcript_api-1.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import csv\n",
        "import os\n",
        "from datetime import datetime\n",
        "from googleapiclient.discovery import build\n",
        "from youtube_transcript_api import YouTubeTranscriptApi"
      ],
      "metadata": {
        "id": "geKb8wOSAcD-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e6e9899",
        "outputId": "6877b69c-499e-486d-fffb-9b4f85fda420"
      },
      "source": [
        "import shutil\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CONFIG\n",
        "# ========================\n",
        "API_KEY = \"AIzaSyDkFs6pVdkR871rDmea_SOyFc5qlmz92NI\"\n",
        "VIDEO_ID = \"bPsAKX1ocTs\"\n",
        "PLAYLIST_ID = \"PLGeL0tgf1NADxLzzVARqadY1H-fNXXkDw\"\n",
        "# Yakult Video: https://www.youtube.com/watch?v=bPsAKX1ocTs\n",
        "# y-HLsEIyDjI\n",
        "\n",
        "# https://www.youtube.com/watch?v=cuoM9amPt-I&list=PLGeL0tgf1NADxLzzVARqadY1H-fNXXkDw"
      ],
      "metadata": {
        "id": "EMYuG3GWAjc-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### get by video"
      ],
      "metadata": {
        "id": "3fpnmNruGKQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FUNCTION: get metadata video\n",
        "# ========================\n",
        "def get_video_info(api_key, video_id):\n",
        "    youtube = build(\"youtube\", \"v3\", developerKey=api_key)\n",
        "\n",
        "    request = youtube.videos().list(\n",
        "        part=\"snippet,statistics,contentDetails,status,player,recordingDetails,topicDetails\",\n",
        "        id=video_id\n",
        "    )\n",
        "    response = request.execute()\n",
        "    if not response[\"items\"]:\n",
        "        return None\n",
        "    return response[\"items\"][0]"
      ],
      "metadata": {
        "id": "QSKFyJ7yAmDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FUNCTION: get transcript\n",
        "# ========================\n",
        "def get_transcript(video_id):\n",
        "    try:\n",
        "        transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['en', 'vi'])\n",
        "        text = \" \".join([line[\"text\"] for line in transcript])\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        return None"
      ],
      "metadata": {
        "id": "AhMlSaXxApcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MAIN PIPELINE\n",
        "# ========================\n",
        "def main():\n",
        "    # 1. get metadata\n",
        "    video_info = get_video_info(API_KEY, VIDEO_ID)\n",
        "    if not video_info:\n",
        "        print(\"‚ùå There is no video.\")\n",
        "        return\n",
        "\n",
        "    snippet = video_info[\"snippet\"]\n",
        "    statistics = video_info.get(\"statistics\", {})\n",
        "    content_details = video_info.get(\"contentDetails\", {})\n",
        "\n",
        "    data = {\n",
        "        \"video_id\": VIDEO_ID,\n",
        "        \"title\": snippet.get(\"title\"),\n",
        "        \"description\": snippet.get(\"description\"),\n",
        "        \"publishedAt\": snippet.get(\"publishedAt\"),\n",
        "        \"channelId\": snippet.get(\"channelId\"),\n",
        "        \"channelTitle\": snippet.get(\"channelTitle\"),\n",
        "        \"tags\": snippet.get(\"tags\", []),\n",
        "        \"viewCount\": statistics.get(\"viewCount\"),\n",
        "        \"likeCount\": statistics.get(\"likeCount\"),\n",
        "        \"commentCount\": statistics.get(\"commentCount\"),\n",
        "        \"duration\": content_details.get(\"duration\"),\n",
        "        \"caption\": content_details.get(\"caption\"),\n",
        "    }\n",
        "\n",
        "    # 2. get transcript\n",
        "    transcript_text = get_transcript(VIDEO_ID)\n",
        "    data[\"transcript\"] = transcript_text if transcript_text else \"No transcript available\"\n",
        "\n",
        "    # Create folder to save data by date\n",
        "    # ========================\n",
        "    today = datetime.today().strftime(\"%Y-%m-%d\")\n",
        "    save_dir = os.path.join(\"data\", today)\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    json_path = os.path.join(save_dir, \"video_data.json\")\n",
        "    csv_path = os.path.join(save_dir, \"video_data.csv\")\n",
        "\n",
        "    # Save JSON\n",
        "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "    # Save CSV\n",
        "    if data:\n",
        "        keys = data.keys()\n",
        "        with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "            writer = csv.DictWriter(f, fieldnames=keys)\n",
        "            writer.writeheader()\n",
        "            writer.writerow(data)\n",
        "\n",
        "    print(f\"‚úÖ Saved data in folder: {save_dir}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qdi6Kr_AXx8",
        "outputId": "ce58f2dd-d2db-49a0-e19e-862d8f1189c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Saved data in folder: data/2025-09-25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "278b3e2d"
      },
      "source": [
        "# Specify the destination folder in your Google Drive\n",
        "drive_save_dir = os.path.join('/content/drive/My Drive/', save_dir)\n",
        "\n",
        "# Create the directory in Google Drive if it doesn't exist\n",
        "os.makedirs(drive_save_dir, exist_ok=True)\n",
        "\n",
        "# Copy the saved files to Google Drive\n",
        "shutil.copy(json_path, drive_save_dir)\n",
        "shutil.copy(csv_path, drive_save_dir)\n",
        "\n",
        "print(f\"‚úÖ Copied data to Google Drive folder: {drive_save_dir}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### get by playlists"
      ],
      "metadata": {
        "id": "Py2hycfOGG86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FUNCTION: get videoId in playlist\n",
        "# ========================\n",
        "def get_videos_from_playlist(api_key, playlist_id):\n",
        "    youtube = build(\"youtube\", \"v3\", developerKey=api_key)\n",
        "    videos = []\n",
        "    next_page_token = None\n",
        "\n",
        "    while True:\n",
        "        request = youtube.playlistItems().list(\n",
        "            part=\"contentDetails\",\n",
        "            playlistId=playlist_id,\n",
        "            maxResults=50,\n",
        "            pageToken=next_page_token\n",
        "        )\n",
        "        response = request.execute()\n",
        "\n",
        "        for item in response[\"items\"]:\n",
        "            videos.append(item[\"contentDetails\"][\"videoId\"])\n",
        "\n",
        "        next_page_token = response.get(\"nextPageToken\")\n",
        "        if not next_page_token:\n",
        "            break\n",
        "    return videos"
      ],
      "metadata": {
        "id": "BZL4mYMfGFGY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FUNCTION: get metadata video\n",
        "# ========================\n",
        "def get_video_info(api_key, video_id):\n",
        "    youtube = build(\"youtube\", \"v3\", developerKey=api_key)\n",
        "    request = youtube.videos().list(\n",
        "        part=\"snippet,statistics,contentDetails,status\",\n",
        "        id=video_id\n",
        "    )\n",
        "    response = request.execute()\n",
        "    if not response[\"items\"]:\n",
        "        return None\n",
        "    return response[\"items\"][0]"
      ],
      "metadata": {
        "id": "AOJ75BUMImsj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FUNCTION: get transcript\n",
        "# ========================\n",
        "def get_transcript(video_id):\n",
        "    try:\n",
        "        transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['en', 'vi'])\n",
        "        text = \" \".join([line[\"text\"] for line in transcript])\n",
        "        return text\n",
        "    except:\n",
        "        return None"
      ],
      "metadata": {
        "id": "7ShDYmdNIu_-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MAIN PIPELINE\n",
        "# ========================\n",
        "def main():\n",
        "    video_ids = get_videos_from_playlist(API_KEY, PLAYLIST_ID)\n",
        "    print(f\"üîπ Found {len(video_ids)} video in playlist\")\n",
        "\n",
        "    all_data = []\n",
        "\n",
        "    for idx, vid in enumerate(video_ids, 1):\n",
        "        print(f\"‚û°Ô∏è Crawl video {idx}/{len(video_ids)}: {vid}\")\n",
        "        video_info = get_video_info(API_KEY, vid)\n",
        "        if not video_info:\n",
        "            continue\n",
        "\n",
        "        snippet = video_info[\"snippet\"]\n",
        "        statistics = video_info.get(\"statistics\", {})\n",
        "        content_details = video_info.get(\"contentDetails\", {})\n",
        "\n",
        "        data = {\n",
        "            \"video_id\": vid,\n",
        "            \"title\": snippet.get(\"title\"),\n",
        "            \"description\": snippet.get(\"description\"),\n",
        "            \"publishedAt\": snippet.get(\"publishedAt\"),\n",
        "            \"channelId\": snippet.get(\"channelId\"),\n",
        "            \"channelTitle\": snippet.get(\"channelTitle\"),\n",
        "            \"tags\": snippet.get(\"tags\", []),\n",
        "            \"viewCount\": statistics.get(\"viewCount\"),\n",
        "            \"likeCount\": statistics.get(\"likeCount\"),\n",
        "            \"commentCount\": statistics.get(\"commentCount\"),\n",
        "            \"duration\": content_details.get(\"duration\"),\n",
        "            \"caption\": content_details.get(\"caption\"),\n",
        "        }\n",
        "\n",
        "        transcript_text = get_transcript(vid)\n",
        "        data[\"transcript\"] = transcript_text if transcript_text else \"No transcript available\"\n",
        "\n",
        "        all_data.append(data)\n",
        "\n",
        "    # Create folder to save data by date\n",
        "    # ========================\n",
        "    today = datetime.today().strftime(\"%Y-%m-%d\")\n",
        "    save_dir = os.path.join(\"data\", today)\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    json_path = os.path.join(save_dir, \"playlist_data.json\")\n",
        "    csv_path = os.path.join(save_dir, \"playlist_data.csv\")\n",
        "\n",
        "    # Save JSON\n",
        "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(all_data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "    # Save CSV\n",
        "    if all_data:\n",
        "        keys = all_data[0].keys()\n",
        "        with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "            writer = csv.DictWriter(f, fieldnames=keys)\n",
        "            writer.writeheader()\n",
        "            writer.writerows(all_data)\n",
        "\n",
        "    print(f\"‚úÖ Saved data in folder: {save_dir}\")\n",
        "\n",
        "    # # Save JSON\n",
        "    # with open(\"playlist_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    #     json.dump(all_data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "    # # Save CSV\n",
        "    # keys = all_data[0].keys() if all_data else []\n",
        "    # with open(\"playlist_data.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    #     writer = csv.DictWriter(f, fieldnames=keys)\n",
        "    #     writer.writeheader()\n",
        "    #     writer.writerows(all_data)\n",
        "\n",
        "    # print(\"‚úÖ Saved playlist_data.json and playlist_data.csv\")\n",
        "\n",
        "    # Specify the destination folder in your Google Drive\n",
        "    drive_save_dir = os.path.join('/content/drive/My Drive/Research/Brand Image Detection/Code', save_dir)\n",
        "\n",
        "    # Create the directory in Google Drive if it doesn't exist\n",
        "    os.makedirs(drive_save_dir, exist_ok=True)\n",
        "\n",
        "    # Copy the saved files to Google Drive\n",
        "    shutil.copy(json_path, drive_save_dir)\n",
        "    shutil.copy(csv_path, drive_save_dir)\n",
        "\n",
        "    print(f\"‚úÖ Copied data to Google Drive folder: {drive_save_dir}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "rrAKsK-FI2jf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f6b729d-111a-4fc6-ab2b-3172444e63f8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîπ Found 59 video in playlist\n",
            "‚û°Ô∏è Crawl video 1/59: cuoM9amPt-I\n",
            "‚û°Ô∏è Crawl video 2/59: 86MV7fCVTs4\n",
            "‚û°Ô∏è Crawl video 3/59: JjnmcSTHQYk\n",
            "‚û°Ô∏è Crawl video 4/59: i459dVnvTi0\n",
            "‚û°Ô∏è Crawl video 5/59: 909igbIvzCA\n",
            "‚û°Ô∏è Crawl video 6/59: w1tuKtHLeEM\n",
            "‚û°Ô∏è Crawl video 7/59: ftPJ6kTSRNc\n",
            "‚û°Ô∏è Crawl video 8/59: pogJoL8oXsY\n",
            "‚û°Ô∏è Crawl video 9/59: pllzbS_vcRk\n",
            "‚û°Ô∏è Crawl video 10/59: VYc-ZwQiEl0\n",
            "‚û°Ô∏è Crawl video 11/59: 9ZQMlI-QBuE\n",
            "‚û°Ô∏è Crawl video 12/59: P55VKCNxdoA\n",
            "‚û°Ô∏è Crawl video 13/59: MtANUOOjuPo\n",
            "‚û°Ô∏è Crawl video 14/59: x0yXt21tTjA\n",
            "‚û°Ô∏è Crawl video 15/59: iS1ibDDozvI\n",
            "‚û°Ô∏è Crawl video 16/59: kSv6ZxkFzGE\n",
            "‚û°Ô∏è Crawl video 17/59: vmBCH5YDjFA\n",
            "‚û°Ô∏è Crawl video 18/59: mYqlmeg6u98\n",
            "‚û°Ô∏è Crawl video 19/59: rvTR4TsYxjk\n",
            "‚û°Ô∏è Crawl video 20/59: -iOm8ydhftk\n",
            "‚û°Ô∏è Crawl video 21/59: cmflrVLacio\n",
            "‚û°Ô∏è Crawl video 22/59: ktCeW0Yy75w\n",
            "‚û°Ô∏è Crawl video 23/59: kjhgDOHHd8Q\n",
            "‚û°Ô∏è Crawl video 24/59: 9BTj8tQOjjg\n",
            "‚û°Ô∏è Crawl video 25/59: _3r7k3D1pps\n",
            "‚û°Ô∏è Crawl video 26/59: WREkt9tTeSQ\n",
            "‚û°Ô∏è Crawl video 27/59: WXpgfSfvh6k\n",
            "‚û°Ô∏è Crawl video 28/59: AkrmCCbp9dE\n",
            "‚û°Ô∏è Crawl video 29/59: vTc9iWgYOAM\n",
            "‚û°Ô∏è Crawl video 30/59: PZumS0PTx9I\n",
            "‚û°Ô∏è Crawl video 31/59: UmCsdw88KfQ\n",
            "‚û°Ô∏è Crawl video 32/59: _-omky3LJ1Q\n",
            "‚û°Ô∏è Crawl video 33/59: n8BUv34FAAA\n",
            "‚û°Ô∏è Crawl video 34/59: s7sBSGhU5nk\n",
            "‚û°Ô∏è Crawl video 35/59: 4_rD3xGV0xs\n",
            "‚û°Ô∏è Crawl video 36/59: 5rM0cuqqzHk\n",
            "‚û°Ô∏è Crawl video 37/59: wRNCN5ieOyQ\n",
            "‚û°Ô∏è Crawl video 38/59: IHEOMoozLbI\n",
            "‚û°Ô∏è Crawl video 39/59: Ufnmqic1w44\n",
            "‚û°Ô∏è Crawl video 40/59: 4v-NxFBxQSY\n",
            "‚û°Ô∏è Crawl video 41/59: -IP0oQyVoOI\n",
            "‚û°Ô∏è Crawl video 42/59: ZsEOVOo9bmA\n",
            "‚û°Ô∏è Crawl video 43/59: YhKdPLDEwzg\n",
            "‚û°Ô∏è Crawl video 44/59: 4aQ4VxBfmAI\n",
            "‚û°Ô∏è Crawl video 45/59: TQNQ_Gd5qGw\n",
            "‚û°Ô∏è Crawl video 46/59: Geq94yrv2No\n",
            "‚û°Ô∏è Crawl video 47/59: pOpL93tW95k\n",
            "‚û°Ô∏è Crawl video 48/59: 58CwGz__rRo\n",
            "‚û°Ô∏è Crawl video 49/59: Bj6eHlBw_BI\n",
            "‚û°Ô∏è Crawl video 50/59: vQsOuhI77TM\n",
            "‚û°Ô∏è Crawl video 51/59: YKBNgzsLIn0\n",
            "‚û°Ô∏è Crawl video 52/59: OqhSy1EfI44\n",
            "‚û°Ô∏è Crawl video 53/59: h-s-pmlXLSs\n",
            "‚û°Ô∏è Crawl video 54/59: e1F2vILFllA\n",
            "‚û°Ô∏è Crawl video 55/59: UeFbn518N70\n",
            "‚û°Ô∏è Crawl video 56/59: jsPP1asNA0o\n",
            "‚û°Ô∏è Crawl video 57/59: Yng-PevPZpE\n",
            "‚û°Ô∏è Crawl video 58/59: aTPWCuRwiuc\n",
            "‚û°Ô∏è Crawl video 59/59: O7wQ36fSQOE\n",
            "‚úÖ Saved data in folder: data/2025-09-26\n",
            "‚úÖ Copied data to Google Drive folder: /content/drive/My Drive/Research/Brand Image Detection/Code/data/2025-09-26\n"
          ]
        }
      ]
    }
  ]
}