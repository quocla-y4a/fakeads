{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WPadQIsAS9b"
      },
      "source": [
        "## Topic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8DUvQXzAeAN",
        "outputId": "1681b032-1703-4779-a5fe-2db061f129a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: youtube_transcript_api in c:\\users\\quocla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.2.3)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in c:\\users\\quocla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from youtube_transcript_api) (0.7.1)\n",
            "Requirement already satisfied: requests in c:\\users\\quocla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from youtube_transcript_api) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\quocla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->youtube_transcript_api) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\quocla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->youtube_transcript_api) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\quocla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->youtube_transcript_api) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\quocla\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->youtube_transcript_api) (2025.8.3)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.2 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "pip install youtube_transcript_api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "geKb8wOSAcD-"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import csv\n",
        "import os\n",
        "from datetime import datetime\n",
        "from googleapiclient.discovery import build\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e6e9899",
        "outputId": "6877b69c-499e-486d-fffb-9b4f85fda420"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mshutil\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      4\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EMYuG3GWAjc-"
      },
      "outputs": [],
      "source": [
        "# CONFIG\n",
        "# ========================\n",
        "API_KEY = \"AIzaSyDkFs6pVdkR871rDmea_SOyFc5qlmz92NI\"\n",
        "VIDEO_ID = \"bPsAKX1ocTs\"\n",
        "PLAYLIST_ID = \"PLqFfyQGXEX5rC-BKJpLY67PT3lDh684M0\"\n",
        "\n",
        "# Playlist Qu·∫£ng c√°o Ph√°t ki·∫øm\n",
        "# https://www.youtube.com/watch?v=IM8lTK9Jbco&list=PLqFfyQGXEX5rC-BKJpLY67PT3lDh684M0\n",
        "\n",
        "# Yakult Video: https://www.youtube.com/watch?v=bPsAKX1ocTs\n",
        "# y-HLsEIyDjI\n",
        "\n",
        "# https://www.youtube.com/watch?v=cuoM9amPt-I&list=PLGeL0tgf1NADxLzzVARqadY1H-fNXXkDw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fpnmNruGKQX"
      },
      "source": [
        "### get by video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QSKFyJ7yAmDK"
      },
      "outputs": [],
      "source": [
        "# FUNCTION: get metadata video\n",
        "# ========================\n",
        "def get_video_info(api_key, video_id):\n",
        "    youtube = build(\"youtube\", \"v3\", developerKey=api_key)\n",
        "\n",
        "    request = youtube.videos().list(\n",
        "        part=\"snippet,statistics,contentDetails,status,player,recordingDetails,topicDetails\",\n",
        "        id=video_id\n",
        "    )\n",
        "    response = request.execute()\n",
        "    if not response[\"items\"]:\n",
        "        return None\n",
        "    return response[\"items\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AhMlSaXxApcv"
      },
      "outputs": [],
      "source": [
        "# FUNCTION: get transcript\n",
        "# ========================\n",
        "def get_transcript(video_id):\n",
        "    try:\n",
        "        transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['en', 'vi'])\n",
        "        text = \" \".join([line[\"text\"] for line in transcript])\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qdi6Kr_AXx8",
        "outputId": "ce58f2dd-d2db-49a0-e19e-862d8f1189c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Saved data in folder: data\\2025-10-21\n"
          ]
        }
      ],
      "source": [
        "# MAIN PIPELINE\n",
        "# ========================\n",
        "def main():\n",
        "    # 1. get metadata\n",
        "    video_info = get_video_info(API_KEY, VIDEO_ID)\n",
        "    if not video_info:\n",
        "        print(\"‚ùå There is no video.\")\n",
        "        return\n",
        "\n",
        "    snippet = video_info[\"snippet\"]\n",
        "    statistics = video_info.get(\"statistics\", {})\n",
        "    content_details = video_info.get(\"contentDetails\", {})\n",
        "\n",
        "    data = {\n",
        "        \"video_id\": VIDEO_ID,\n",
        "        \"title\": snippet.get(\"title\"),\n",
        "        \"description\": snippet.get(\"description\"),\n",
        "        \"publishedAt\": snippet.get(\"publishedAt\"),\n",
        "        \"channelId\": snippet.get(\"channelId\"),\n",
        "        \"channelTitle\": snippet.get(\"channelTitle\"),\n",
        "        \"tags\": snippet.get(\"tags\", []),\n",
        "        \"viewCount\": statistics.get(\"viewCount\"),\n",
        "        \"likeCount\": statistics.get(\"likeCount\"),\n",
        "        \"commentCount\": statistics.get(\"commentCount\"),\n",
        "        \"duration\": content_details.get(\"duration\"),\n",
        "        \"caption\": content_details.get(\"caption\"),\n",
        "    }\n",
        "\n",
        "    # 2. get transcript\n",
        "    transcript_text = get_transcript(VIDEO_ID)\n",
        "    data[\"transcript\"] = transcript_text if transcript_text else \"No transcript available\"\n",
        "\n",
        "    # Create folder to save data by date\n",
        "    # ========================\n",
        "    today = datetime.today().strftime(\"%Y-%m-%d\")\n",
        "    save_dir = os.path.join(\"data\", today)\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    json_path = os.path.join(save_dir, \"video_data.json\")\n",
        "    csv_path = os.path.join(save_dir, \"video_data.csv\")\n",
        "\n",
        "    # Save JSON\n",
        "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "    # Save CSV\n",
        "    if data:\n",
        "        keys = data.keys()\n",
        "        with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "            writer = csv.DictWriter(f, fieldnames=keys)\n",
        "            writer.writeheader()\n",
        "            writer.writerow(data)\n",
        "\n",
        "    print(f\"‚úÖ Saved data in folder: {save_dir}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "278b3e2d"
      },
      "outputs": [],
      "source": [
        "# Specify the destination folder in your Google Drive\n",
        "drive_save_dir = os.path.join('/content/drive/My Drive/', save_dir)\n",
        "\n",
        "# Create the directory in Google Drive if it doesn't exist\n",
        "os.makedirs(drive_save_dir, exist_ok=True)\n",
        "\n",
        "# Copy the saved files to Google Drive\n",
        "shutil.copy(json_path, drive_save_dir)\n",
        "shutil.copy(csv_path, drive_save_dir)\n",
        "\n",
        "print(f\"‚úÖ Copied data to Google Drive folder: {drive_save_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Py2hycfOGG86"
      },
      "source": [
        "### get by playlists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BZL4mYMfGFGY"
      },
      "outputs": [],
      "source": [
        "# FUNCTION: get videoId in playlist\n",
        "# ========================\n",
        "def get_videos_from_playlist(api_key, playlist_id):\n",
        "    youtube = build(\"youtube\", \"v3\", developerKey=api_key)\n",
        "    videos = []\n",
        "    next_page_token = None\n",
        "\n",
        "    while True:\n",
        "        request = youtube.playlistItems().list(\n",
        "            part=\"contentDetails\",\n",
        "            playlistId=playlist_id,\n",
        "            maxResults=50,\n",
        "            pageToken=next_page_token\n",
        "        )\n",
        "        response = request.execute()\n",
        "\n",
        "        for item in response[\"items\"]:\n",
        "            videos.append(item[\"contentDetails\"][\"videoId\"])\n",
        "\n",
        "        next_page_token = response.get(\"nextPageToken\")\n",
        "        if not next_page_token:\n",
        "            break\n",
        "    return videos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "AOJ75BUMImsj"
      },
      "outputs": [],
      "source": [
        "# FUNCTION: get metadata video\n",
        "# ========================\n",
        "def get_video_info(api_key, video_id):\n",
        "    youtube = build(\"youtube\", \"v3\", developerKey=api_key)\n",
        "    request = youtube.videos().list(\n",
        "        part=\"snippet,statistics,contentDetails,status\",\n",
        "        id=video_id\n",
        "    )\n",
        "    response = request.execute()\n",
        "    if not response[\"items\"]:\n",
        "        return None\n",
        "    return response[\"items\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7ShDYmdNIu_-"
      },
      "outputs": [],
      "source": [
        "# FUNCTION: get transcript\n",
        "# ========================\n",
        "def get_transcript(video_id):\n",
        "    try:\n",
        "        transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['en', 'vi'])\n",
        "        text = \" \".join([line[\"text\"] for line in transcript])\n",
        "        return text\n",
        "    except:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import time\n",
        "\n",
        "def get_video_comments(api_key, video_id, max_results=100):\n",
        "    # Get all comments\n",
        "    comments = []\n",
        "    base_url = \"https://www.googleapis.com/youtube/v3/commentThreads\"\n",
        "    params = {\n",
        "        \"part\": \"snippet,replies\",\n",
        "        \"videoId\": video_id,\n",
        "        \"maxResults\": max_results,\n",
        "        \"textFormat\": \"plainText\",\n",
        "        \"key\": api_key\n",
        "    }\n",
        "\n",
        "    while True:\n",
        "        response = requests.get(base_url, params=params)\n",
        "        if response.status_code != 200:\n",
        "            print(f\"‚ùå Error fetching comments for {video_id}: {response.text}\")\n",
        "            break\n",
        "\n",
        "        data = response.json()\n",
        "\n",
        "        for item in data.get(\"items\", []):\n",
        "            top_comment_snippet = item[\"snippet\"][\"topLevelComment\"][\"snippet\"]\n",
        "            top_comment = {\n",
        "                \"author\": top_comment_snippet.get(\"authorDisplayName\"),\n",
        "                \"text\": top_comment_snippet.get(\"textDisplay\"),\n",
        "                \"likeCount\": top_comment_snippet.get(\"likeCount\"),\n",
        "                \"publishedAt\": top_comment_snippet.get(\"publishedAt\"),\n",
        "                \"replies\": []\n",
        "            }\n",
        "\n",
        "            # Get replies\n",
        "            replies = item.get(\"replies\", {}).get(\"comments\", [])\n",
        "            for reply in replies:\n",
        "                reply_snippet = reply[\"snippet\"]\n",
        "                top_comment[\"replies\"].append({\n",
        "                    \"author\": reply_snippet.get(\"authorDisplayName\"),\n",
        "                    \"text\": reply_snippet.get(\"textDisplay\"),\n",
        "                    \"likeCount\": reply_snippet.get(\"likeCount\"),\n",
        "                    \"publishedAt\": reply_snippet.get(\"publishedAt\"),\n",
        "                })\n",
        "\n",
        "            comments.append(top_comment)\n",
        "\n",
        "        # Next page\n",
        "        if \"nextPageToken\" in data:\n",
        "            params[\"pageToken\"] = data[\"nextPageToken\"]\n",
        "            time.sleep(0.3)\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return comments\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrAKsK-FI2jf",
        "outputId": "2f6b729d-111a-4fc6-ab2b-3172444e63f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîπ Found 49 video in playlist\n",
            "‚û°Ô∏è Crawl video 1/49: IM8lTK9Jbco\n",
            "‚û°Ô∏è Crawl video 2/49: PEF-Fm_qlLM\n",
            "‚û°Ô∏è Crawl video 3/49: 62LO6w-x3mA\n",
            "‚û°Ô∏è Crawl video 4/49: V2oIm3UsDWA\n",
            "‚û°Ô∏è Crawl video 5/49: 3Mi4dasrMs8\n",
            "‚û°Ô∏è Crawl video 6/49: yX4USW1ayN0\n",
            "‚û°Ô∏è Crawl video 7/49: HpAl-gLNX2o\n",
            "‚û°Ô∏è Crawl video 8/49: 6qK2L0FIk7I\n",
            "‚û°Ô∏è Crawl video 9/49: FobeSoa8lPQ\n",
            "‚û°Ô∏è Crawl video 10/49: 74zcRMxB55o\n",
            "‚û°Ô∏è Crawl video 11/49: zlbQOuQCC9s\n",
            "‚û°Ô∏è Crawl video 12/49: GIbW0s65bvo\n",
            "‚û°Ô∏è Crawl video 13/49: XTQefN01Xdc\n",
            "‚û°Ô∏è Crawl video 14/49: _KcIV6YPi24\n",
            "‚û°Ô∏è Crawl video 15/49: U9BkxisKcBE\n",
            "‚û°Ô∏è Crawl video 16/49: 3icFJbnIa3o\n",
            "‚û°Ô∏è Crawl video 17/49: QhcACjV6tV4\n",
            "‚û°Ô∏è Crawl video 18/49: Z_Jhwh4Gs7c\n",
            "‚û°Ô∏è Crawl video 19/49: O4yft2fOMF0\n",
            "‚û°Ô∏è Crawl video 20/49: yuOU05qgHeM\n",
            "‚û°Ô∏è Crawl video 21/49: xeEspos2D1w\n",
            "‚û°Ô∏è Crawl video 22/49: CcV8mDM_-wE\n",
            "‚û°Ô∏è Crawl video 23/49: v3rjOqu9Lzs\n",
            "‚û°Ô∏è Crawl video 24/49: Vo46XSvc_EM\n",
            "‚û°Ô∏è Crawl video 25/49: M0yLS31IgG0\n",
            "‚û°Ô∏è Crawl video 26/49: mRef76-6MF0\n",
            "‚û°Ô∏è Crawl video 27/49: 1jB69K4OGLU\n",
            "‚û°Ô∏è Crawl video 28/49: 2aOYRXSAzUk\n",
            "‚û°Ô∏è Crawl video 29/49: SsyF4snKSzg\n",
            "‚û°Ô∏è Crawl video 30/49: nF8gTf_oSEA\n",
            "‚û°Ô∏è Crawl video 31/49: Ijv7LkWH65Q\n",
            "‚û°Ô∏è Crawl video 32/49: LMC_NtOLCb8\n",
            "‚û°Ô∏è Crawl video 33/49: vS-GrN-bMeA\n",
            "‚û°Ô∏è Crawl video 34/49: 9l1YY9NmubE\n",
            "‚û°Ô∏è Crawl video 35/49: JhsbpPK--zQ\n",
            "‚û°Ô∏è Crawl video 36/49: 2GXL-gRTqcs\n",
            "‚û°Ô∏è Crawl video 37/49: J_2JFZTbhtM\n",
            "‚û°Ô∏è Crawl video 38/49: arplbMg1TP4\n",
            "‚û°Ô∏è Crawl video 39/49: -orM7KpZz7s\n",
            "‚û°Ô∏è Crawl video 40/49: viAq7FbFxJA\n",
            "‚û°Ô∏è Crawl video 41/49: WS-4Ia0_yNw\n",
            "‚û°Ô∏è Crawl video 42/49: iw5crHlWLvI\n",
            "‚û°Ô∏è Crawl video 43/49: ihty6h3MQPg\n",
            "‚û°Ô∏è Crawl video 44/49: GE_UkazQ3VE\n",
            "‚û°Ô∏è Crawl video 45/49: 3pCzxdMSQyw\n",
            "‚û°Ô∏è Crawl video 46/49: qFxYkVieBGo\n",
            "‚û°Ô∏è Crawl video 47/49: ctmiS1fve9o\n",
            "‚û°Ô∏è Crawl video 48/49: kNRe2UensdU\n",
            "‚ùå Error fetching comments for kNRe2UensdU: {\n",
            "  \"error\": {\n",
            "    \"code\": 403,\n",
            "    \"message\": \"The video identified by the \\u003ccode\\u003e\\u003ca href=\\\"/youtube/v3/docs/commentThreads/list#videoId\\\"\\u003evideoId\\u003c/a\\u003e\\u003c/code\\u003e parameter has disabled comments.\",\n",
            "    \"errors\": [\n",
            "      {\n",
            "        \"message\": \"The video identified by the \\u003ccode\\u003e\\u003ca href=\\\"/youtube/v3/docs/commentThreads/list#videoId\\\"\\u003evideoId\\u003c/a\\u003e\\u003c/code\\u003e parameter has disabled comments.\",\n",
            "        \"domain\": \"youtube.commentThread\",\n",
            "        \"reason\": \"commentsDisabled\",\n",
            "        \"location\": \"videoId\",\n",
            "        \"locationType\": \"parameter\"\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\n",
            "‚û°Ô∏è Crawl video 49/49: ewDyegoHic0\n",
            "‚úÖ Saved data in folder: data\\2025-10-31\n"
          ]
        }
      ],
      "source": [
        "# MAIN PIPELINE\n",
        "# ========================\n",
        "def main():\n",
        "    video_ids = get_videos_from_playlist(API_KEY, PLAYLIST_ID)\n",
        "    print(f\"üîπ Found {len(video_ids)} video in playlist\")\n",
        "\n",
        "    all_data = []\n",
        "\n",
        "    for idx, vid in enumerate(video_ids, 1):\n",
        "        print(f\"‚û°Ô∏è Crawl video {idx}/{len(video_ids)}: {vid}\")\n",
        "        video_info = get_video_info(API_KEY, vid)\n",
        "        if not video_info:\n",
        "            continue\n",
        "\n",
        "        snippet = video_info[\"snippet\"]\n",
        "        statistics = video_info.get(\"statistics\", {})\n",
        "        content_details = video_info.get(\"contentDetails\", {})\n",
        "\n",
        "        # Get data\n",
        "        data = {\n",
        "            \"video_id\": vid,\n",
        "            \"title\": snippet.get(\"title\"),\n",
        "            \"description\": snippet.get(\"description\"),\n",
        "            \"publishedAt\": snippet.get(\"publishedAt\"),\n",
        "            \"channelId\": snippet.get(\"channelId\"),\n",
        "            \"channelTitle\": snippet.get(\"channelTitle\"),\n",
        "            \"tags\": snippet.get(\"tags\", []),\n",
        "            \"viewCount\": statistics.get(\"viewCount\"),\n",
        "            \"likeCount\": statistics.get(\"likeCount\"),\n",
        "            \"commentCount\": statistics.get(\"commentCount\"),\n",
        "            \"duration\": content_details.get(\"duration\"),\n",
        "            \"caption\": content_details.get(\"caption\"),\n",
        "        }\n",
        "\n",
        "        transcript_text = get_transcript(vid)\n",
        "        data[\"transcript\"] = transcript_text if transcript_text else \"No transcript available\"\n",
        "\n",
        "        comments = get_video_comments(API_KEY, vid)\n",
        "        data[\"comments\"] = comments if comments else []\n",
        "\n",
        "        all_data.append(data)\n",
        "\n",
        "    # Create folder to save data by date\n",
        "    # ========================\n",
        "    today = datetime.today().strftime(\"%Y-%m-%d\")\n",
        "    save_dir = os.path.join(\"data\", today)\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    json_path = os.path.join(save_dir, \"playlist_data.json\")\n",
        "    csv_path = os.path.join(save_dir, \"playlist_data.csv\")\n",
        "\n",
        "    # Save JSON\n",
        "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(all_data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "    # Save CSV\n",
        "    if all_data:\n",
        "        keys = all_data[0].keys()\n",
        "        with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "            writer = csv.DictWriter(f, fieldnames=keys)\n",
        "            writer.writeheader()\n",
        "            writer.writerows(all_data)\n",
        "\n",
        "    print(f\"‚úÖ Saved data in folder: {save_dir}\")\n",
        "\n",
        "    # # Save JSON\n",
        "    # with open(\"playlist_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    #     json.dump(all_data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "    # # Save CSV\n",
        "    # keys = all_data[0].keys() if all_data else []\n",
        "    # with open(\"playlist_data.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    #     writer = csv.DictWriter(f, fieldnames=keys)\n",
        "    #     writer.writeheader()\n",
        "    #     writer.writerows(all_data)\n",
        "\n",
        "    # print(\"‚úÖ Saved playlist_data.json and playlist_data.csv\")\n",
        "\n",
        "    # Specify the destination folder in your Google Drive -----\n",
        "    # drive_save_dir = os.path.join('/content/drive/My Drive/Research/Brand Image Detection/Code', save_dir)\n",
        "\n",
        "    # # Create the directory in Google Drive if it doesn't exist\n",
        "    # os.makedirs(drive_save_dir, exist_ok=True)\n",
        "\n",
        "    # # Copy the saved files to Google Drive\n",
        "    # shutil.copy(json_path, drive_save_dir)\n",
        "    # shutil.copy(csv_path, drive_save_dir)\n",
        "\n",
        "    # print(f\"‚úÖ Copied data to Google Drive folder: {drive_save_dir}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
